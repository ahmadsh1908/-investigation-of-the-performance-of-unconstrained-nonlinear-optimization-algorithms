# Investigation of the performance of unconstrained nonlinear optimization algorithms

This is a component of the project specifications for the IOE 511 course, which focuses on Continuous Optimization Methods and is offered at the University of Michigan, Ann Arbor. 

Our task was to apply the following algorithms to address ten distinct problem sets outlined in the Project_Problems.pdf document. These problem sets included both quadratic, exponential, trigonometric and other non-linear optimization functions.

The implemented algorithms included: a. Gradient Descent - (utilizing Wolfe Line Search and Backtracking Line Search) b. Modified Newton  Method - (employing Wolfe Line Search and Backtracking Line Search) c. Trust region Newton with Conjugate Gradient subproblem solver d. Symmetric Rank 1 Method with Quasi-Newton, using a Conjugate Gradient subproblem solver e. BFGS quasi-Newton - (utilizing Wolfe Line Search and Backtracking Line Search) g. DFP quasi-Newton - (employing Wolfe Line Search and Backtracking Line Search)

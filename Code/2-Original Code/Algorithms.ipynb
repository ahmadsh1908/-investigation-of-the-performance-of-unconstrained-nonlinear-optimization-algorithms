{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1679495458977,
     "user": {
      "displayName": "Ahmad Fadel Shmayssani Saleh",
      "userId": "00105995564892959118"
     },
     "user_tz": 240
    },
    "id": "gbBSuCJaOYmL"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ArmijoLineSearch(x, f, d, g, c1, problem, method, options):\n",
    "    \"\"\"\n",
    "    Perform an Armijo line search to find a step size that satisfies the Armijo condition.\n",
    "\n",
    "    Args:\n",
    "        x (numpy.ndarray): Current iterate.\n",
    "        f (float): Current objective function value.\n",
    "        d (numpy.ndarray): Search direction.\n",
    "        g (numpy.ndarray): Gradient of the objective function.\n",
    "        c1 (float): Armijo condition parameter.\n",
    "        problem (Problem): Problem object defining the optimization problem.\n",
    "        method (Method): Method object defining optimization method settings.\n",
    "        options (Options): Options object defining optimization options.\n",
    "\n",
    "    Returns:\n",
    "        float: The step size that satisfies the Armijo condition.\n",
    "        int: Number of iterations taken.\n",
    "    \"\"\"\n",
    "    alpha = method.step_size  # Initialize the step size to a pre-defined value\n",
    "    tau = options.tau  # Define a reduction factor for the step size\n",
    "    counter = 0\n",
    "\n",
    "    while problem.compute_f(x + alpha * d) > (f + (c1 * alpha * np.dot(g.T, d))):\n",
    "        alpha = alpha * options.tau  # Update alpha until the Armijo condition is satisfied\n",
    "        counter += 1\n",
    "\n",
    "    return alpha, counter\n",
    "\n",
    "def Wolfe(x, f, d, g, c1, c2, problem, method, options):\n",
    "    \"\"\"\n",
    "    Perform a Wolfe line search to find a step size that satisfies both Armijo and curvature conditions.\n",
    "\n",
    "    Args:\n",
    "        x (numpy.ndarray): Current iterate.\n",
    "        f (float): Current objective function value.\n",
    "        d (numpy.ndarray): Search direction.\n",
    "        g (numpy.ndarray): Gradient of the objective function.\n",
    "        c1 (float): Armijo condition parameter.\n",
    "        c2 (float): Curvature condition parameter.\n",
    "        problem (Problem): Problem object defining the optimization problem.\n",
    "        method (Method): Method object defining optimization method settings.\n",
    "        options (Options): Options object defining optimization options.\n",
    "\n",
    "    Returns:\n",
    "        float: The step size that satisfies both Armijo and curvature conditions.\n",
    "        int: Number of iterations taken.\n",
    "    \"\"\"\n",
    "    alpha = method.step_size  # Initialize the step size to a pre-defined value\n",
    "    tau = options.tau  # Define a reduction factor for the step size\n",
    "    counter = 0\n",
    "\n",
    "    while problem.compute_f(x + alpha * d) > (problem.compute_f(x) + (c1 * alpha * np.dot(g.T, d))) \\\n",
    "            and np.abs(np.dot(problem.compute_g(x + alpha * d), d)) > c2*np.abs(np.dot(problem.compute_g(x), d)):\n",
    "        alpha = alpha * options.tau  # Reduce the step size by multiplying it with the reduction factor\n",
    "        counter += 1\n",
    "\n",
    "    return alpha, counter\n",
    "\n",
    "def Weak_Wolfe(x, f, d, g, c1, c2, problem, method, options):\n",
    "    \"\"\"\n",
    "    Perform a Weak Wolfe line search to find a step size that satisfies the Armijo condition and a weak curvature condition.\n",
    "\n",
    "    Args:\n",
    "        x (numpy.ndarray): Current iterate.\n",
    "        f (float): Current objective function value.\n",
    "        d (numpy.ndarray): Search direction.\n",
    "        g (numpy.ndarray): Gradient of the objective function.\n",
    "        c1 (float): Armijo condition parameter.\n",
    "        c2 (float): Curvature condition parameter.\n",
    "        problem (Problem): Problem object defining the optimization problem.\n",
    "        method (Method): Method object defining optimization method settings.\n",
    "        options (Options): Options object defining optimization options.\n",
    "\n",
    "    Returns:\n",
    "        float: The step size that satisfies both Armijo and weak curvature conditions.\n",
    "    \"\"\"\n",
    "    alphabar = method.step_size  # Initialize the step size to a pre-defined value\n",
    "    alpha_high = 1000\n",
    "    alpha_low = 0\n",
    "    c = 0.5\n",
    "    counter = 0\n",
    "    tau = options.tau  # Define a reduction factor for the step size\n",
    "\n",
    "    while True:\n",
    "        if problem.compute_f(x + alphabar * d) <= (problem.compute_f(x) + (c1 * alphabar * np.dot(g.T, d))):\n",
    "            if np.dot(problem.compute_g(x + alphabar * d), d) >= c2*np.dot(problem.compute_g(x), d):\n",
    "                alpha = alphabar  # Step size set if both conditions are satisfied, and break the loop\n",
    "                break\n",
    "\n",
    "        if problem.compute_f(x + alphabar * d) <= (problem.compute_f(x) + (c1 * alphabar * np.dot(g.T, d))):\n",
    "            alpha_low = alphabar\n",
    "        else:\n",
    "            alpha_high = alphabar\n",
    "\n",
    "        alphabar = (c * alpha_low) + (1 - c) * alpha_high  # Update alphabar using thresholds\n",
    "\n",
    "    return alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GDStep(x, f, g, problem, method, options):\n",
    "    \"\"\"\"\n",
    "    This function implements Gradient Descent for unconstrained optimization problems. It sets the descent direction to be \n",
    "    the negative gradient, gets step size using either wolfe or armijo condition and then update based on the step size found\n",
    "    and returns new point x, function value and gradient at new point.\n",
    "\n",
    "    The function returns the updated values of x, f, g\n",
    "    Args:\n",
    "    x (numpy.ndarray): Initial guess for the solution\n",
    "    f (float): Value of the function at x\n",
    "    g (numpy.ndarray): Gradient of the function at x\n",
    "    problem (Problem): Object representing the optimization problem to be solved\n",
    "    method (Method): Object representing the optimization method to use\n",
    "    options (Options): Object containing the options for the optimization method\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: Solution at the next iteration\n",
    "    float: Value of the function at the next iteration\n",
    "    numpy.ndarray: Gradient of the function at the next iteration\n",
    "    \"\"\"\n",
    "\n",
    "    d = -g  # Set the search direction d to be -gradient\n",
    "\n",
    "    # Perform a line search using the specified step type\n",
    "    if method.step_type == 'Backtracking':\n",
    "        c1 = options.c_1_ls  # c1 for line search\n",
    "        alpha, counter = ArmijoLineSearch(\n",
    "            x, f, d, g, c1, problem, method, options)  # call armijo line search\n",
    "    elif method.step_type == 'Wolfe':\n",
    "        c1 = options.c_1_ls  # c1 for line search\n",
    "        c2 = options.c_2_ls  # c2 for line search\n",
    "        # Call wolfe conditions to get alpha\n",
    "        alpha, counter = Wolfe(x, f, d, g, c1, c2, problem, method, options)\n",
    "    else:\n",
    "        print('Warning: step type is not defined')\n",
    "\n",
    "    # Update the position, function value, and gradient using the step size alpha and search direction d\n",
    "    x_new = x + alpha*d\n",
    "    f_new = problem.compute_f(x_new)\n",
    "    g_new = problem.compute_g(x_new)\n",
    "\n",
    "    # Return the updated position, function value, and gradient\n",
    "    return x_new, f_new, g_new, counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Newton(x, f, g, H, problem, method, options):\n",
    "    \"\"\"\n",
    "    This function implements Newton method for unconstrained optimization problems.The Cholesky function is used to compute the \n",
    "    Cholesky decomposition of the Hessian matrix H. The parameter beta is used to add a regularization term to the diagonal \n",
    "    of H if necessary to ensure that H is positive definite. The Newton step direction d is computed by solving the linear\n",
    "    system L@L.T@d = -g, where L is the Cholesky decomposition of H. \n",
    "\n",
    "The function returns the updated values of x, f, g, and H.\n",
    "    Args:\n",
    "    x (numpy.ndarray): Initial guess for the solution\n",
    "    f (float): Value of the function at x\n",
    "    g (numpy.ndarray): Gradient of the function at x\n",
    "    H (np.ndarray) : Hessian of function at point x0\n",
    "    problem (Problem): Object representing the optimization problem to be solved\n",
    "    method (Method): Object representing the optimization method to use\n",
    "    options (Options): Object containing the options for the optimization method\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: Solution at the next iteration\n",
    "    float: Value of the function at the next iteration\n",
    "    numpy.ndarray: Gradient of the function at the next iteration\n",
    "    numpy.ndarray: Hessian at xnew which is x+(alpha*d)\n",
    "    \"\"\"\n",
    "\n",
    "    # Define beta and the function Cholesky that computes the Cholesky decomposition of a matrix with a regularization parameter beta\n",
    "    beta = options.B\n",
    "  # Cholesky subroutine\n",
    "    n = H.shape[0]\n",
    "    # Find the minimum diagonal element of A\n",
    "    diag_min = np.min(np.diag(H))\n",
    "    if diag_min > 0:\n",
    "        eta = 0\n",
    "    else:\n",
    "        eta = -1 * diag_min + options.B\n",
    "    # Perform the Cholesky decomposition with the regularized matrix A + eta*I until it succeeds, or until the maximum number of iterations is reached\n",
    "    for k in range(1000):\n",
    "        try:\n",
    "            L = np.linalg.cholesky(H + eta * np.identity(n))\n",
    "            break\n",
    "        except:\n",
    "            eta = np.max([2 * eta, options.B])\n",
    "   # Compute the search direction d as the solution of the Newton system H*d = -g using the Cholesky decomposition\n",
    "    d = np.matmul(np.linalg.inv(H + eta * np.identity(n)), -1 * g)  # step size\n",
    "\n",
    "    # Perform the line search to find the step size alpha\n",
    "    if method.step_type == 'Backtracking':\n",
    "        c1 = options.c_1_ls\n",
    "        alpha, counter = ArmijoLineSearch(\n",
    "            x, f, d, g, c1, problem, method, options)\n",
    "\n",
    "    elif method.step_type == 'Wolfe':\n",
    "        c1 = options.c_1_ls\n",
    "        c2 = options.c_2_ls\n",
    "        alpha, counter = Wolfe(x, f, d, g, c1, c2, problem, method, options)\n",
    "    else:\n",
    "        print('Warning: step type is not defined')\n",
    "\n",
    "    # Update the variables with the new values obtained by taking a step in the search direction with the step size alpha\n",
    "    x_new = x + alpha * d\n",
    "    f_new = problem.compute_f(x_new)\n",
    "    g_new = problem.compute_g(x_new)\n",
    "    H_new = problem.compute_H(x_new)\n",
    "\n",
    "    # Return the updated values of x, f, g, and H\n",
    "    return x_new, f_new, g_new, H_new, counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BFGS(x, f, g, H_inv, problem, method, options):\n",
    "    \"\"\"\n",
    "    This function implements the Broyden-Fletcher-Goldfarb-Shanno algorithm  for unconstrained \n",
    "    optimization problems.The algorithm builds a quasi-Newton approximation to the Hessian matrix H by updating it iteratively. \n",
    "\n",
    "    Args:\n",
    "    x (numpy.ndarray): Initial guess for the solution\n",
    "    f (float): Value of the function at x\n",
    "    g (numpy.ndarray): Gradient of the function at x\n",
    "    H (np.ndarray) : Hessian starts with identity and then is the update of each BFGS iteration\n",
    "    problem (Problem): Object representing the optimization problem to be solved\n",
    "    method (Method): Object representing the optimization method to use\n",
    "    options (Options): Object containing the options for the optimization method\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: Solution at the next iteration\n",
    "    float: Value of the function at the next iteration\n",
    "    numpy.ndarray: Gradient of the function at the next iteration\n",
    "    numpy.ndarray: Hessian at next iteration following BFGS update or no update\n",
    "    list: Updated list of previous displacement vectors\n",
    "    list: Updated list of previous gradient differences\n",
    "    \"\"\"\n",
    "\n",
    "    # Set the search direction d to be -H*g, where H is the approximate inverse Hessian matrix\n",
    "    # and g is the gradient vector\n",
    "    d = -H_inv @ g\n",
    "\n",
    "    if method.step_type == 'Backtracking':\n",
    "        # Perform backtracking line search to find the step size alpha that satisfies the Armijo condition\n",
    "        c1 = options.c_1_ls  # constant for Armijo condition\n",
    "        alpha, c = ArmijoLineSearch(x, f, d, g, c1, problem, method, options)\n",
    "\n",
    "    elif method.step_type == 'Wolfe':\n",
    "        # Perform Wolfe line search to find the step size alpha that satisfies both Armijo and curvature conditions\n",
    "        c1 = options.c_1_ls  # constant for Armijo condition\n",
    "        c2 = options.c_2_ls  # constant for curvature condition\n",
    "        alpha, c = Wolfe(x, f, d, g, c1, c2, problem, method, options)\n",
    "\n",
    "    else:\n",
    "        # If an unsupported line search method is used, print a warning message\n",
    "        print('Warning: step type is not defined')\n",
    "\n",
    "    # Update x, f, and g based on the chosen step size alpha and search direction d\n",
    "    x_new = x + alpha * d\n",
    "    f_new = problem.compute_f(x_new)\n",
    "    g_new = problem.compute_g(x_new)\n",
    "\n",
    "    # define s_k and y_k\n",
    "    s_k = x_new - x\n",
    "    y_k = g_new - g\n",
    "    n = H_inv.shape[0]\n",
    "\n",
    "    # update H inverse approximation if s_k.T @ y_k is suffficently PSD\n",
    "    if s_k.T @ y_k >= options.term_tol * np.linalg.norm(s_k, ord=2) * np.linalg.norm(y_k, ord=2):\n",
    "        rho_k = 1 / (y_k.T @ s_k)\n",
    "        H_inv_new = (np.identity(n) - rho_k * np.outer(s_k, y_k)) @ H_inv @ (\n",
    "            np.identity(n) - rho_k * np.outer(y_k, s_k)) + rho_k * np.outer(s_k, s_k)\n",
    "    else:\n",
    "        # print('skipped update!')\n",
    "        H_inv_new = H_inv\n",
    "\n",
    "    # Return the updated values of x, f, g, and H, as well as the change in x and g\n",
    "    return x_new, f_new, g_new, H_inv_new, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DFP(x, f, g, H_inv, problem, method, options):\n",
    "    \"\"\"\n",
    "    This function implements the DFP (Davidon-Fletcher-Powell) algorithm for unconstrained optimization problems.\n",
    "    The algorithm builds a quasi-Newton approximation to the Hessian matrix H by updating it iteratively. \n",
    "\n",
    "    Args:\n",
    "    x (numpy.ndarray): Initial guess for the solution\n",
    "    f (float): Value of the function at x\n",
    "    g (numpy.ndarray): Gradient of the function at x\n",
    "    H (np.ndarray) : Hessian starts with identity and then is the update of each DFP iteration\n",
    "    problem (Problem): Object representing the optimization problem to be solved\n",
    "    method (Method): Object representing the optimization method to use\n",
    "    options (Options): Object containing the options for the optimization method\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: Solution at the next iteration\n",
    "    float: Value of the function at the next iteration\n",
    "    numpy.ndarray: Gradient of the function at the next iteration\n",
    "    numpy.ndarray: Hessian at next iteration following DFP update or no update\n",
    "    \"\"\"\n",
    "\n",
    "  # Set the search direction d to be -g\n",
    "    # Set the search direction d to be -H*g, where H is the approximate inverse Hessian matrix\n",
    "    # and g is the gradient vector\n",
    "    d = -H_inv @ g\n",
    "\n",
    "    if method.step_type == 'Backtracking':\n",
    "        # Perform backtracking line search to find the step size alpha that satisfies the Armijo condition\n",
    "        c1 = options.c_1_ls  # constant for Armijo condition\n",
    "        alpha, c = ArmijoLineSearch(x, f, d, g, c1, problem, method, options)\n",
    "\n",
    "    elif method.step_type == 'Wolfe':\n",
    "        # Perform Wolfe line search to find the step size alpha that satisfies both Armijo and curvature conditions\n",
    "        c1 = options.c_1_ls  # constant for Armijo condition\n",
    "        c2 = options.c_2_ls  # constant for curvature condition\n",
    "        alpha, c = Wolfe(x, f, d, g, c1, c2, problem, method, options)\n",
    "\n",
    "    else:\n",
    "        # If an unsupported line search method is used, print a warning message\n",
    "        print('Warning: step type is not defined')\n",
    "\n",
    "    # Update x, f, and g based on the chosen step size alpha and search direction d\n",
    "    x_new = x + alpha * d\n",
    "    f_new = problem.compute_f(x_new)\n",
    "    g_new = problem.compute_g(x_new)\n",
    "\n",
    "    # define s_k and y_k\n",
    "    s_k = x_new - x\n",
    "    y_k = g_new - g\n",
    "    n = H_inv.shape[0]\n",
    "\n",
    "    # update H inverse approximation if s_k.T @ y_k is suffficently PSD\n",
    "    if s_k.T @ y_k >= options.term_tol * np.linalg.norm(s_k, ord=2) * np.linalg.norm(y_k, ord=2):\n",
    "        rho = 1 / np.dot(s_k, y_k)\n",
    "        term1 = H_inv\n",
    "        term2 = (H_inv@np.outer(y_k, y_k)@H_inv) / \\\n",
    "            (np.dot(np.matmul(y_k, H_inv), y_k))\n",
    "        term3 = (np.outer(s_k, s_k)) / (np.dot(s_k, y_k))\n",
    "        H_inv_new = term1 - term2 + term3\n",
    "    else:  # If no sufficient increase hessian not updated.\n",
    "        H_inv_new = H_inv\n",
    "\n",
    "    # Return the updated values\n",
    "    return x_new, f_new, g_new, H_inv_new, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trust Region Subproblem Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CG_Steihaug(x, f, g, H, Delta, problem, method, options):\n",
    "    \"\"\"\n",
    "    The function CG_Steihaug implements the conjugate gradient method with the Steihaug stopping criterion\n",
    "    for solving the quadratic subproblem in the trust-region method.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "    x: the current iterate (array)\n",
    "    f: the value of the objective function at the current iterate (float)\n",
    "    g: the gradient of the objective function at the current iterate (array)\n",
    "    H: the Hessian of the objective function at the current iterate (2D array)\n",
    "    Delta: the trust-region radius (float)\n",
    "    problem: an object that defines the optimization problem being solved (class)\n",
    "    method: an object that defines the optimization method being used (class)\n",
    "    options: an object that contains options for the optimization algorithm (class)\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    dk: the solution to the trust-region subproblem (array)\n",
    "    \"\"\"\n",
    "\n",
    "    def quadratic_formula(z, p, Delta):\n",
    "        p_root = (-2 * z @ p + np.sqrt((-2 * z @ p)**2 - 4 * np.linalg.norm(p)\n",
    "                  ** 2 * (np.linalg.norm(z)**2 - Delta**2))) / (2 * np.linalg.norm(p)**2)\n",
    "\n",
    "        return p_root\n",
    "\n",
    "    z = np.zeros(len(x))  # Initialize z to 0 vector\n",
    "    r = g  # Initialize r to the gradient vector\n",
    "    p, Bk = -g.copy(), H  # Initialize p and Bk to -r and Hessian matrix respectively\n",
    "\n",
    "    r_l2norm = np.linalg.norm(r, ord=2)  # Compute l2-norm of r\n",
    "\n",
    "    if r_l2norm < options.term_tol_CG:  # Check if norm of r is smaller than tolerance\n",
    "        dk = z  # If yes, set dk to 0 vector and return\n",
    "        return dk\n",
    "\n",
    "    for j in range(options.max_iterations_CG):  # Loop over maximum number of iterations\n",
    "        if p.T @ Bk @ p <= 0:  # Check if p^T Bk p is non-positive\n",
    "            # Find τ such that dk = zj + τpj minimizes mk (dk ) and satisfies ||dk|| = ∆k\n",
    "            # Compute dk using QuadraticFormula and set to dk\n",
    "            dk = z+(p*quadratic_formula(z, p, Delta))\n",
    "            return dk  # Return dk\n",
    "\n",
    "        alpha = r.T @ r / (p.T @ Bk @ p)  # Compute step size alpha\n",
    "\n",
    "        z_next = z + alpha * p  # Compute new value of z\n",
    "\n",
    "        # Check if norm of z_next is greater than Delta\n",
    "        if np.linalg.norm(z_next, ord=2) >= Delta:\n",
    "            # Find τ such that dk = zj + τpj minimizes mk (dk ) and satisfies kdkk = ∆k\n",
    "            # Compute dk using QuadraticFormula and set to dk\n",
    "            dk = z+(p*quadratic_formula(z, p, Delta))\n",
    "            # print('greater than delta')\n",
    "            return dk  # Return dk\n",
    "\n",
    "        r_next = r + alpha * (Bk @ p)  # Compute new value of r\n",
    "\n",
    "        # Check if norm of r_next is smaller than tolerance\n",
    "        if np.linalg.norm(r_next) <= options.term_tol_CG:\n",
    "            dk = z_next  # If yes, set dk to z_next and return\n",
    "            return dk\n",
    "\n",
    "        B_next = (r_next@r_next) / (r@r)  # Compute new value of B_next\n",
    "        # print(B_next.shape)\n",
    "        p_next = -r_next + (B_next*p)  # Compute new value of p_next\n",
    "\n",
    "        r = r_next\n",
    "        p = p_next\n",
    "        z = z_next  # Update r, p and z to r_next, p_next and z_next respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trust Region Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TRNewtonCG(x, f, g, H, delta, problem, method, options):\n",
    "    \"\"\"\n",
    "    TRNewtonCG is a function that implements the trust-region Newton-CG optimization method for minimizing a\n",
    "    given objective function subject to constraints, using the CG-Steihaug method to solve the trust-region subproblem.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "    x: the initial guess for the optimization variables\n",
    "    f: the objective function to be minimized\n",
    "    g: the gradient of the objective function\n",
    "    H: the Hessian matrix of the objective function\n",
    "    delta: the size of the trust-region\n",
    "    problem: an object that defines the problem to be solved\n",
    "    method: a string indicating the optimization method to be used\n",
    "    options: an object that contains options for the optimization method\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    x_new: the new iterate of the optimization variables\n",
    "    f_new: the value of the objective function at x_new\n",
    "    g_new: the gradient of the objective function at x_new\n",
    "    H_new: the Hessian matrix of the objective function at x_new\n",
    "    delta_new: the updated size of the trust-region.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set the trust region c1 and c2 parameters\n",
    "    c1 = options.c_1_tr\n",
    "    c2 = options.c_2_tr\n",
    "\n",
    "    # Solve the Trust Region subproblem approximately using the Conjugate Gradient method\n",
    "    dk = CG_Steihaug(x, f, g, H, delta, problem, method, options)\n",
    "    # print(dk)\n",
    "\n",
    "    # Compute the numerator in the ratio\n",
    "    numerator = problem.compute_f(x) - problem.compute_f(x+dk)\n",
    "\n",
    "    # Compute the denominator in the ratio using the quadratic approximation model\n",
    "    d0 = np.zeros((len(x)))\n",
    "    # model = lambda d: f + g.T @ d + .5*( d.T @ H @ d)\n",
    "    denominator = (f + g.T @ d0 + .5*(d0.T @ H @ d0)) - \\\n",
    "        (f + g.T @ dk + .5*(dk.T @ H @ dk))\n",
    "\n",
    "    # Compute the ratio\n",
    "    rho = numerator / denominator\n",
    "\n",
    "    # Accept or reject the step based on the ratio and update the trust region radius\n",
    "    if rho > c1:\n",
    "        x_new = x + dk\n",
    "        delta_new = delta\n",
    "\n",
    "        f_new = problem.compute_f(x_new)\n",
    "        g_new = problem.compute_g(x_new)\n",
    "        H_new = problem.compute_H(x_new)\n",
    "\n",
    "        if rho > c2:\n",
    "            delta_new = 2 * delta\n",
    "    else:\n",
    "        x_new, f_new, g_new,  H_new = x, f, g, H\n",
    "        delta_new = .5 * delta\n",
    "\n",
    "    # Return the new point and the updated trust region radius\n",
    "    return x_new, f_new, g_new, H_new, delta_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TRSR1CG(x, f, g, H, delta, problem, method, options):\n",
    "    \"\"\"\n",
    "    his function implements the Trust-Region Symmetric Rank-1 (TRSR1) method for solving unconstrained optimization problems.\n",
    "    using the CG-Steihaug method to solve the trust-region subproblem.\n",
    "\n",
    "     Arguments:\n",
    "\n",
    "     x: the initial guess for the optimization variables\n",
    "     f: the objective function to be minimized\n",
    "     g: the gradient of the objective function\n",
    "     H: Hessian is initially the identity and then updated based on Rank 1 Update.\n",
    "     delta: the size of the trust-region\n",
    "     problem: an object that defines the problem to be solved\n",
    "     method: a string indicating the optimization method to be used\n",
    "     options: an object that contains options for the optimization method\n",
    "\n",
    "     Returns:\n",
    "\n",
    "     x_new: the new iterate of the optimization variables\n",
    "     f_new: the value of the objective function at x_new\n",
    "     g_new: the gradient of the objective function at x_new\n",
    "     H_new: the Hessian matrix of the objective function at x_new\n",
    "     delta_new: the updated size of the trust-region.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize variables\n",
    "    Bk = H\n",
    "    r = options.r\n",
    "    eta = 1e-3\n",
    "    tol = options.term_tol_CG\n",
    "    counter = 0\n",
    "\n",
    "    x_list = []\n",
    "    f_list = []\n",
    "    c_list = []\n",
    "\n",
    "    while np.linalg.norm(g) > tol and counter < 50:\n",
    "        dk = CG_Steihaug(x, f, g, H, delta, problem, method, options)\n",
    "\n",
    "        # Compute actual and predicted reduction\n",
    "        yk = problem.compute_g(x + dk) - problem.compute_g(x)\n",
    "        ared = problem.compute_f(x) - problem.compute_f(x + dk)\n",
    "        pred = -((problem.compute_g(x).T @ dk) + 0.5 * (dk.T @ Bk @ dk))\n",
    "\n",
    "        # Check if the step is acceptable\n",
    "        if ared / pred > eta:\n",
    "            x_new = x + dk\n",
    "        else:\n",
    "            x_new = x\n",
    "\n",
    "        # Update trust region radius\n",
    "        if ared / pred > 0.75:\n",
    "            if np.linalg.norm(dk) <= 0.8 * delta:\n",
    "                delta_new = delta\n",
    "            else:\n",
    "                delta_new = 2 * delta\n",
    "        elif 0.1 <= ared / pred <= 0.75:\n",
    "            delta_new = delta\n",
    "        else:\n",
    "            delta_new = 0.5 * delta\n",
    "\n",
    "        # Update Bk using SR1\n",
    "        term = yk - (Bk @ dk)\n",
    "        if np.abs(dk.T @ term) >= r * np.linalg.norm(dk) * np.linalg.norm(term):\n",
    "            Bk_new = Bk + (np.outer(term, term)) / (term.T @ dk)\n",
    "        else:\n",
    "            Bk_new = Bk\n",
    "\n",
    "        # Update variables\n",
    "        x = x_new\n",
    "        f = problem.compute_f(x_new)\n",
    "        g = problem.compute_g(x_new)\n",
    "        H = problem.compute_H(x_new)\n",
    "        Bk = Bk_new\n",
    "        delta = delta_new\n",
    "        counter += 1\n",
    "\n",
    "        # update lists\n",
    "        x_list.append(x)\n",
    "        f_list.append(f)\n",
    "\n",
    "    return x, f, x_list, f_list, c_list"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPwZXGIYHcZ5XDJBjlz2hXI",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
